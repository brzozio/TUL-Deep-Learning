{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.models import resnet18\n",
    "from torchmetrics.classification import BinaryAccuracy\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import pytorch_lightning as pl\n",
    "from models import FaceID_CNN, Ready_faceID_CNN\n",
    "import os\n",
    "from torchvision import datasets\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "base_path : str = os.path.dirname(os.getcwd())\n",
    "CSV_PATH  : str = base_path + '\\\\csv'\n",
    "src_path  : str = base_path + '\\\\src'\n",
    "json_path : str = base_path + '\\\\json'\n",
    "\n",
    "traits = ['Male']\n",
    "\n",
    "\n",
    "class CelebADataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, img_folder, labels, transform=None):\n",
    "        self.img_folder = img_folder\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.img_folder, self.labels.iloc[idx][\"image_id\"])\n",
    "        label = self.labels.iloc[idx][1:].values.astype(\"float32\")\n",
    "        image = Image.open(img_name).convert(\"RGB\")\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, torch.tensor(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 68261, Validation size: 19867, Test size: 19962\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "        transforms.Resize((128, 128)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    "\n",
    "dataset_path     = os.path.join(CSV_PATH, \"celeba\")\n",
    "img_folder_path  = os.path.join(dataset_path, \"img_align_celeba\")\n",
    "attr_path        = os.path.join(dataset_path, \"list_attr_celeba.txt\")\n",
    "partition_path   = os.path.join(dataset_path, \"list_eval_partition.txt\")\n",
    "\n",
    "if not os.path.exists(img_folder_path) or not os.path.exists(attr_path) or not os.path.exists(partition_path):\n",
    "    raise FileNotFoundError(\"The dataset folder or required files are missing.\")\n",
    "\n",
    "attr_df = pd.read_csv(attr_path, sep=r'\\s+', skiprows=1)\n",
    "attr_df = attr_df.reset_index().rename(columns={\"index\": \"image_id\"})\n",
    "attr_df[\"image_id\"] = attr_df[\"image_id\"].astype(str)\n",
    "\n",
    "attr_df         = attr_df[[\"image_id\"] + traits]\n",
    "\n",
    "attr_df[traits] = (attr_df[traits] + 1) // 2\n",
    "\n",
    "filtered_df     = attr_df[(attr_df[traits] > 0).any(axis=1)]\n",
    "\n",
    "partition_df = pd.read_csv(partition_path, sep=' ', header=None, names=[\"image_id\", \"partition\"])\n",
    "partition_df[\"image_id\"] = partition_df[\"image_id\"].astype(str)\n",
    "\n",
    "filtered_df = filtered_df.merge(partition_df, on=\"image_id\")\n",
    "non_filtered_df = attr_df.merge(partition_df, on=\"image_id\")\n",
    "\n",
    "train_df = filtered_df[filtered_df[\"partition\"] == 0].drop(columns=[\"partition\"])\n",
    "val_df   = non_filtered_df[non_filtered_df[\"partition\"] == 1].drop(columns=[\"partition\"])\n",
    "test_df  = non_filtered_df[non_filtered_df[\"partition\"] == 2].drop(columns=[\"partition\"])\n",
    "\n",
    "print(f\"Train size: {len(train_df)}, Validation size: {len(val_df)}, Test size: {len(test_df)}\")\n",
    "\n",
    "if len(train_df) == 0 or len(val_df) == 0 or len(test_df) == 0:\n",
    "    raise ValueError(\"One of the dataset splits is empty. Check your data files and partition file.\")\n",
    "\n",
    "train_data = CelebADataset(img_folder_path, train_df, transform)\n",
    "val_data   = CelebADataset(img_folder_path, val_df, transform)\n",
    "test_data  = CelebADataset(img_folder_path, test_df, transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr_df[traits] = (attr_df[traits] + 1) // 2\n",
    "filtered_df = attr_df[(attr_df[traits] > 0).any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68261\n"
     ]
    }
   ],
   "source": [
    "print(len(train_df[\"Male\"] == 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>Male</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000001.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000002.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000003.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000004.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000005.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202594</th>\n",
       "      <td>202595.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202595</th>\n",
       "      <td>202596.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202596</th>\n",
       "      <td>202597.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202597</th>\n",
       "      <td>202598.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202598</th>\n",
       "      <td>202599.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>202599 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          image_id  Male\n",
       "0       000001.jpg     0\n",
       "1       000002.jpg     0\n",
       "2       000003.jpg     1\n",
       "3       000004.jpg     0\n",
       "4       000005.jpg     0\n",
       "...            ...   ...\n",
       "202594  202595.jpg     0\n",
       "202595  202596.jpg     1\n",
       "202596  202597.jpg     1\n",
       "202597  202598.jpg     0\n",
       "202598  202599.jpg     0\n",
       "\n",
       "[202599 rows x 2 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State: model.model.conv1.weight\n",
      "State: model.model.bn1.weight\n",
      "State: model.model.bn1.bias\n",
      "State: model.model.bn1.running_mean\n",
      "State: model.model.bn1.running_var\n",
      "State: model.model.bn1.num_batches_tracked\n",
      "State: model.model.layer1.0.conv1.weight\n",
      "State: model.model.layer1.0.bn1.weight\n",
      "State: model.model.layer1.0.bn1.bias\n",
      "State: model.model.layer1.0.bn1.running_mean\n",
      "State: model.model.layer1.0.bn1.running_var\n",
      "State: model.model.layer1.0.bn1.num_batches_tracked\n",
      "State: model.model.layer1.0.conv2.weight\n",
      "State: model.model.layer1.0.bn2.weight\n",
      "State: model.model.layer1.0.bn2.bias\n",
      "State: model.model.layer1.0.bn2.running_mean\n",
      "State: model.model.layer1.0.bn2.running_var\n",
      "State: model.model.layer1.0.bn2.num_batches_tracked\n",
      "State: model.model.layer1.1.conv1.weight\n",
      "State: model.model.layer1.1.bn1.weight\n",
      "State: model.model.layer1.1.bn1.bias\n",
      "State: model.model.layer1.1.bn1.running_mean\n",
      "State: model.model.layer1.1.bn1.running_var\n",
      "State: model.model.layer1.1.bn1.num_batches_tracked\n",
      "State: model.model.layer1.1.conv2.weight\n",
      "State: model.model.layer1.1.bn2.weight\n",
      "State: model.model.layer1.1.bn2.bias\n",
      "State: model.model.layer1.1.bn2.running_mean\n",
      "State: model.model.layer1.1.bn2.running_var\n",
      "State: model.model.layer1.1.bn2.num_batches_tracked\n",
      "State: model.model.layer2.0.conv1.weight\n",
      "State: model.model.layer2.0.bn1.weight\n",
      "State: model.model.layer2.0.bn1.bias\n",
      "State: model.model.layer2.0.bn1.running_mean\n",
      "State: model.model.layer2.0.bn1.running_var\n",
      "State: model.model.layer2.0.bn1.num_batches_tracked\n",
      "State: model.model.layer2.0.conv2.weight\n",
      "State: model.model.layer2.0.bn2.weight\n",
      "State: model.model.layer2.0.bn2.bias\n",
      "State: model.model.layer2.0.bn2.running_mean\n",
      "State: model.model.layer2.0.bn2.running_var\n",
      "State: model.model.layer2.0.bn2.num_batches_tracked\n",
      "State: model.model.layer2.0.downsample.0.weight\n",
      "State: model.model.layer2.0.downsample.1.weight\n",
      "State: model.model.layer2.0.downsample.1.bias\n",
      "State: model.model.layer2.0.downsample.1.running_mean\n",
      "State: model.model.layer2.0.downsample.1.running_var\n",
      "State: model.model.layer2.0.downsample.1.num_batches_tracked\n",
      "State: model.model.layer2.1.conv1.weight\n",
      "State: model.model.layer2.1.bn1.weight\n",
      "State: model.model.layer2.1.bn1.bias\n",
      "State: model.model.layer2.1.bn1.running_mean\n",
      "State: model.model.layer2.1.bn1.running_var\n",
      "State: model.model.layer2.1.bn1.num_batches_tracked\n",
      "State: model.model.layer2.1.conv2.weight\n",
      "State: model.model.layer2.1.bn2.weight\n",
      "State: model.model.layer2.1.bn2.bias\n",
      "State: model.model.layer2.1.bn2.running_mean\n",
      "State: model.model.layer2.1.bn2.running_var\n",
      "State: model.model.layer2.1.bn2.num_batches_tracked\n",
      "State: model.model.layer3.0.conv1.weight\n",
      "State: model.model.layer3.0.bn1.weight\n",
      "State: model.model.layer3.0.bn1.bias\n",
      "State: model.model.layer3.0.bn1.running_mean\n",
      "State: model.model.layer3.0.bn1.running_var\n",
      "State: model.model.layer3.0.bn1.num_batches_tracked\n",
      "State: model.model.layer3.0.conv2.weight\n",
      "State: model.model.layer3.0.bn2.weight\n",
      "State: model.model.layer3.0.bn2.bias\n",
      "State: model.model.layer3.0.bn2.running_mean\n",
      "State: model.model.layer3.0.bn2.running_var\n",
      "State: model.model.layer3.0.bn2.num_batches_tracked\n",
      "State: model.model.layer3.0.downsample.0.weight\n",
      "State: model.model.layer3.0.downsample.1.weight\n",
      "State: model.model.layer3.0.downsample.1.bias\n",
      "State: model.model.layer3.0.downsample.1.running_mean\n",
      "State: model.model.layer3.0.downsample.1.running_var\n",
      "State: model.model.layer3.0.downsample.1.num_batches_tracked\n",
      "State: model.model.layer3.1.conv1.weight\n",
      "State: model.model.layer3.1.bn1.weight\n",
      "State: model.model.layer3.1.bn1.bias\n",
      "State: model.model.layer3.1.bn1.running_mean\n",
      "State: model.model.layer3.1.bn1.running_var\n",
      "State: model.model.layer3.1.bn1.num_batches_tracked\n",
      "State: model.model.layer3.1.conv2.weight\n",
      "State: model.model.layer3.1.bn2.weight\n",
      "State: model.model.layer3.1.bn2.bias\n",
      "State: model.model.layer3.1.bn2.running_mean\n",
      "State: model.model.layer3.1.bn2.running_var\n",
      "State: model.model.layer3.1.bn2.num_batches_tracked\n",
      "State: model.model.layer4.0.conv1.weight\n",
      "State: model.model.layer4.0.bn1.weight\n",
      "State: model.model.layer4.0.bn1.bias\n",
      "State: model.model.layer4.0.bn1.running_mean\n",
      "State: model.model.layer4.0.bn1.running_var\n",
      "State: model.model.layer4.0.bn1.num_batches_tracked\n",
      "State: model.model.layer4.0.conv2.weight\n",
      "State: model.model.layer4.0.bn2.weight\n",
      "State: model.model.layer4.0.bn2.bias\n",
      "State: model.model.layer4.0.bn2.running_mean\n",
      "State: model.model.layer4.0.bn2.running_var\n",
      "State: model.model.layer4.0.bn2.num_batches_tracked\n",
      "State: model.model.layer4.0.downsample.0.weight\n",
      "State: model.model.layer4.0.downsample.1.weight\n",
      "State: model.model.layer4.0.downsample.1.bias\n",
      "State: model.model.layer4.0.downsample.1.running_mean\n",
      "State: model.model.layer4.0.downsample.1.running_var\n",
      "State: model.model.layer4.0.downsample.1.num_batches_tracked\n",
      "State: model.model.layer4.1.conv1.weight\n",
      "State: model.model.layer4.1.bn1.weight\n",
      "State: model.model.layer4.1.bn1.bias\n",
      "State: model.model.layer4.1.bn1.running_mean\n",
      "State: model.model.layer4.1.bn1.running_var\n",
      "State: model.model.layer4.1.bn1.num_batches_tracked\n",
      "State: model.model.layer4.1.conv2.weight\n",
      "State: model.model.layer4.1.bn2.weight\n",
      "State: model.model.layer4.1.bn2.bias\n",
      "State: model.model.layer4.1.bn2.running_mean\n",
      "State: model.model.layer4.1.bn2.running_var\n",
      "State: model.model.layer4.1.bn2.num_batches_tracked\n",
      "State: model.model.fc.0.weight\n",
      "State: model.model.fc.0.bias\n",
      "State: model.model.fc.2.weight\n",
      "State: model.model.fc.2.bias\n",
      "State: criterion.pos_weight\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "base_path : str = os.path.dirname(os.getcwd())\n",
    "model_path : str = base_path + \"\\\\models\"\n",
    "checkpoint = torch.load(model_path+\"\\\\torch.ckpt\")\n",
    "\n",
    "for state in checkpoint[\"state_dict\"]:\n",
    "    print(f\"State: {state}\") \n",
    "\n",
    "# Remove \"criterion.pos_weight\" from state_dict\n",
    "if \"criterion.pos_weight\" in checkpoint[\"state_dict\"]:\n",
    "    del checkpoint[\"state_dict\"][\"criterion.pos_weight\"]\n",
    "\n",
    "# Save the cleaned checkpoint\n",
    "torch.save(checkpoint, model_path+\"\\\\torch.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "base_path : str = os.path.dirname(os.getcwd())\n",
    "annotations_path : str = base_path + \"\\\\csv\\\\processed_wider_faces\"\n",
    "test = pd.read_json(annotations_path + \"\\\\annotations_DONE.json\")\n",
    "test[\"attributes\"][0][\"Smiling\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 21\u001b[0m\n\u001b[0;32m     19\u001b[0m idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m idx \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(lines):\n\u001b[1;32m---> 21\u001b[0m         relative_image_path \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelpath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages_folder\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m         annotation_image_path \u001b[38;5;241m=\u001b[39m lines[idx]\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     24\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m annotation_image_path \u001b[38;5;241m==\u001b[39m relative_image_path:\n",
      "File \u001b[1;32m<frozen ntpath>:774\u001b[0m, in \u001b[0;36mrelpath\u001b[1;34m(path, start)\u001b[0m\n",
      "File \u001b[1;32m<frozen ntpath>:66\u001b[0m, in \u001b[0;36mnormcase\u001b[1;34m(s)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "base_path : str = os.path.dirname(os.getcwd())\n",
    "DATA_PATH = os.path.join(base_path, \"csv\")\n",
    "images_folder = os.path.join(DATA_PATH, f\"widerface\\\\WIDER_train\\\\WIDER_train\\\\images\")\n",
    "annotations = os.path.join(DATA_PATH, f\"widerface\\\\wider_face_split\\\\wider_face_train_bbx_gt.txt\")\n",
    "\n",
    "\n",
    "imgs = []\n",
    "bounding_boxes = []\n",
    "with open(annotations, 'r') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "for root, dirs, files in os.walk(images_folder):\n",
    "    for filename in files:\n",
    "        if filename.endswith('.jpg'):\n",
    "            image_bboxes = []\n",
    "            idx = 0\n",
    "            while idx < len(lines):\n",
    "                    relative_image_path = os.path.relpath(os.path.join(root, filename), images_folder)\n",
    "                    annotation_image_path = lines[idx].strip()\n",
    "\n",
    "                    if annotation_image_path == relative_image_path:\n",
    "                        num_faces = int(lines[idx + 1].strip())\n",
    "                        idx += 2\n",
    "                        for _ in range(num_faces):\n",
    "                            bbox = list(map(int, lines[idx].strip().split()[:4]))\n",
    "                            x, y, w, h = bbox\n",
    "                            image_bboxes.append([x, y, x + w, y + h])\n",
    "                            idx += 1\n",
    "\n",
    "                        bounding_boxes.append(image_bboxes)\n",
    "                    else:\n",
    "                        idx += 1\n",
    "bounding_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
